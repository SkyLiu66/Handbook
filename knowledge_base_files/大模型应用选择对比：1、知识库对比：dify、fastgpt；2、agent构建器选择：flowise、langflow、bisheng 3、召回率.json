{
    "code": 10000,
    "data": " Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. Thank you. is a kind of... It has already drawn the language into a mathematical structure. So, it's inevitable, including OpenAI, and also every quantum model, the quantum model of OpenAI is relatively common. It's not like some quantum models in the field. For example, some keywords and some sentences have a higher life-saving power. So, when you encounter problems in OpenAI, no matter how you match it, its similarity or its distance is actually 0.7 or more. So, basically, it's hard to make a difference. And then, when you test it, you find that the longer your text is, and when there are more messy contents, its overall life-saving power is not very high. So, when we design it, we try to cut off the parts that are not as easy to understand. We try to cut off the parts that are too much, like the unnecessary parts in a text. And then we try to reduce the content of the lock, to reduce the length of its comparison. Of course, this also has some limitations. The problem it causes is that you will lose some details or some expressive expressions. Yes. Of course, we will have some new structures to solve this problem later. For example, the number of characters, and recently, Lan Xuan has also proposed the multi-type, which is to combine the keywords, the problem, and the tools. We will consider some more diverse combinations later. It's not just about the number of characters, but also about the number of characters. So, we will try to make it more complex. And then, we will try to make it more complex. We will try to make it more complex. And then, we will try to make it more complex. It's not only a simple one-round quantity. OK. Actually, how do you feel about this? As for quantity, I have a friend who asked me, the service provider has a big difference in the capacity of the service provider. Yes. Yes, the service provider is definitely different. Actually, if you want to make it as high as possible, you can just do a little bit of fine tuning. But the premise is that you have to make the fine tuning. Make a small model. If you have a very high fish, you can use a open-air model to feed the fish. It will be very good. But it is not easy. I understand. Actually, now I heard that there is a saying, that is, at the moment, maybe when everyone is choosing this unbounding, that is to say, choosing different unbounding algorithms is still very different, right? Yes. This is certain. Yes, I heard that it seems to be for different verticals, for example, different scenarios, maybe this kind of data and other vertical data, then using different unbounding methods, the result may be completely different. It seems that in the actual operation, many people may use this kind of switching unbounding methods to do this, which is equivalent to increasing the life expectancy. Yes. This is actually, yes, actually, changing the model is the easiest. Yes, because once your training data is changed, it will be a very good comparison scene. If you have some standards on the top line, you can use some standard data to enhance its training. Then this kind of scene will definitely be good. Yes. But there are also limitations. You need to find so much data for each scene. It's also very difficult. Yes. In the future, have you considered, for example, in the unbounding part, to use multiple unbounding and make a distinction for the scene? There are some. In the future, there will be a model market, which is specifically for the user to choose the model. Yes. Yes. Yes. Yes. Yes. Yes. Yes. So, you can choose your own model, whether it is a sample model or an LL model. You can choose your own model. I see. Regarding this, the multi-round dialogue just mentioned, is it not good? We all know that multi-round dialogue is actually different in different model performance. Maybe the domestic one is only about three rounds. Then, like OpenAI, it may reach six or seven rounds. In this case, we are in FaaS GPT. Is there any future or current or any future for this aspect or the future of the FaaS GPT? Is there any consideration in the promotion of multi-round dialogue? Yes. Yes. The promotion of multi-round dialogue, you mean the knowledge library or the knowledge library? Yes, the knowledge library. Because now, if you use the knowledge library, there is a big embarrassment. For example, I have worked with many customers before. After uploading the document, I found that after asking three rounds or four rounds, I was not following the process. I was not following the process. He just talked about the FaaS. In this case, this is actually for us, many of the tools in the knowledge library, we should see if there are any future products or some considerations in this aspect. You can also make a conclusion. In fact, there are several ideas. One is that we should predict the dialogue process because I don't want to make it too bad at the moment. Maybe in the future, we will make a scene for Wenda. For example, you want a document to be summarized and studied, we won't optimize it. And for the scene for Wenda, there may be some steps and some designs. We can let the user design the dialogue in a way that is more or less realistic. And then, we can make some changes to the dialogue and the dialogue in the dialogue. And then, we can make some changes to the dialogue and the dialogue in the dialogue. When the game isержive a Safe Area seamless I will probably give you a good answer. So, from the user's perspective, it's acceptable. When it's finished, we can go back to the original main process. We try to preserve and make the entire preview process from beginning to end, and not leave the preview process. I understand. I understand that the overall idea is that we should put the multiple options into a separate sub-conference, right? It's like making a split. I understand. It should be a preview. A preview. Yes, a preview for a certain scene. When solving a problem, I may not need so much information. At first, I need to know what information the user needs, and what he needs to tell me. I only need to preview these scenes. I understand. Okay. Okay, we have a question. Thank you, Mr. Yu, for sharing a very important topic. This is a topic that many people who work in the knowledge warehouse are concerned about. This topic should be used in the real-time knowledge warehouse, or in the process of RIG scenarios. Everyone is facing a specific real problem. I also look forward to the presentation of this in the future. Okay, next, please. Mr. Qin, please. Let's talk about the limitation of the knowledge warehouse. What are the limitations? And what are the key points that cause these limitations? How do we improve it? Is there any way to solve it? Okay, Mr. Qin, please. Okay, thank you. I would like to thank Mr. Yu first. Because we have seen FastDBT, and it has developed very well. It is also one of the motivation for us to open up our products. So I would like to thank Mr. Yu's team for making such good products. And also give us some directions to learn and approach. This is the first one. And also thank Mr. Zhan for helping us promote and promote RIG for the whole lifetime. He helped us introduce it to everyone. And then, from our experience, as a knowledge warehouse, like Mr. Yu said, we can answer the questions, get the big model, and get the relevant information. To put it bluntly, as a startup company, or as a small team, we may not have the ability to optimize the big model itself. We may be able to use some of the language in the field to make some minor adjustments. And the cost of minor adjustments is actually not bad. This should be able to run. And then, we can use some language in the field to make some minor adjustments. If we can do that, we can measure the volume of the language. As long as we get a 1% or 20% improvement, we can do it. And then, if the recruitment team gets the relevant section, there will be obvious answers. Especially for those who say, I'm going to check something from this document. For example, what is the price? Or what is the requirement of the standard? Something like that. He is... The accuracy is still quite high. Even if you don't have the basic model, you can get a large model with the general model, the accuracy rate can reach a percent... The top level of these open model is at least 70% or 80%. Of course, it has something to do with the difficulty of the language. I'm just asking for a rough estimate. And then, if it involves some more reasoning, it's not just about finding something from a book. It also needs to analyze it. I should go... If it's a more complicated intention, it might be more difficult. At present, we don't have any detailed data for testing. My personal understanding of this is only about 50-60%. Because it requires reasoning. Especially if there is some non-common reasoning, it's more difficult to infer some business know-how. So, in this part, we may have to discuss how to direct it through agents or some business know-how or reasoning logic. This is from the model point of view. In addition, in terms of recall, as Mr. Yu mentioned, the recall strategy of Marty using multiple-way recall is a trend. And then, what we are doing now is... Because, first of all, I think that the quantity search is a simple and rough method. And many people have analyzed it before. It's just a tool. There is no need to evaluate it. It's just another form of expressing information. But in fact, recalling this thing is actually very difficult. Because we humans... Let's say I have 10,000 articles. You can find them. You can't find them. You may not be able to recall the whole content. So, this problem itself is very difficult. Even if we construct and optimize this model, it's difficult to verify or train it. So, it may be difficult to do this. It's very difficult to do this. So, it's very difficult to do this. So, it's very difficult to do this. So, it's very difficult to do this. It is very difficult to have a very high accuracy as we expected. This is the first one. And then, the second one is... The problem of multiple routes that I just mentioned. One of the solutions. We are currently supporting a method that integrates quantity search with ES keyword search. We found that this will be extremely useful for pure quantity search. Because it is especially... Its core logic is that many of them, especially in companies, because our main product is for the enterprise market, it's not a kind of scene where we go to give a demo to the C-level users to play with. We are more for the enterprise. And then, the database of the enterprise products or some more professional scenes like mobile production reports. In fact, there is a lot of information in the enterprise. It's more standardized. So, the keyword, for example, the name of the product, is the core thing to be recruited, rather than some more linguistic ones. Because quantity search is more of a linguistic match. So, we will integrate these two strategies. Including our next step, we will also enhance the point that... Because we have a lot of... We were doing the smart document direction before. So, we have some... We are going to specifically... We are going to take the information and integrate it into this model. In our project, there are limited operating systems. Many bank customers want to manage their non-integrated data. The simplest way to do that is... Most of them are scan cards. What are the pictures of my scan cards? Maybe the full identification is the thickest layer. Then, on this basis, I have to paste more raw data on it. For example, who is the lender? How much is his loan? What is his base? The base is... What is the asset value? Something like that. We can get all of these things out. Based on this information, when I search again, I can take this raw data and do a screening. This will further improve its accuracy. And it will be more suitable for the enterprise market. For some scenes in the enterprise market. This is also one of the directions that we call multi-task integration. And then, this thing might be used to do some scanning with the data library. And then, there are some details. Except for the big point of recalling, there are some details. For example, after we recall, we can do some screening in the recall content. Because it might be the correct answer in the content. But because we recall too much, it is mixed in there. It leads to a bad response. If we remove the irrelevant ones, it will improve its efficiency. So, there are some details like this. We will put some optimization tests in there. We will put this in our open source... Because we can bring a picture to Toradrive and put it in our component. This is the recall part. And then, in fact, the data processing is still very important to influence the entire search and answer effect of the knowledge library. Some of the more common solutions we have now are to cut with the digital. And then, we can do the data processing. And then, we can do the data processing. And then, we can do the data processing. And then, we can use the cross-linked tag to do some cutting. But in fact, the biggest problem... For example, the table. The table is particularly like the cross-page table. The table in the upper page and the table in the lower page are definitely two chunks. It will lead to a lot of language... The data itself won't be found in the table. Because it's cut. So, we are in the stage of data processing. It's also a strength of our history. We are doing OCR. We are doing traditional NLP technology. So, there are a lot of table analysis capabilities in it. We will cut these data according to a more linguistic structure. From one title to the next title, it's a trunk. And then, the whole table is a trunk. This will help more efficient models to acquire a complete information system. Yes, that's about it. Thank you, Mr. Bi. Mr. Bi, what you just mentioned is still very important. It's also the time when everyone is improving the limitations of multi-person dialogue. And the issue of recalling the problem in life. Especially, the recall part. Mr. Bi just mentioned a lot of things. Because they used to do it in the direction of LMP. So, they still have some experience in this area. It should be integrated into the future. I think it's a good idea. Thank you. And I think we'll have a lot of new experience in the future. OK, the next question is quite sharp. I'm going to give a Q&A for both of you. You think there are many products in the market that are not supposed to be asked like this. But I think this is also a problem that everyone will encounter when choosing a knowledge library. Although it's a bit sharp, I still have to give a Q&A for both of you. So, first of all, I'll start with Mr. Yu. For FaaS GPC, what do you think is the biggest advantage of the same product on the market? Or what's the difference? I hope you can explain it to everyone. OK, thank you. Welcome, Mr. Yu. I'm too embarrassed to say this. OK. I think the biggest one is our fun. It's more fun. It's more powerful. It includes the entire bridge. Basically, the bridges that can be exposed have been exposed. Whether you can see the complete top and bottom, or you can do feedback, and see what apps are there, and the specific location of your prompt, it's like you don't have any blind spots in the prediction. Sometimes, these predictions can't be changed. The user can't tell us what to do. We want to make all these predictions so that the user can make the changes. Yes, I think the biggest advantage is... Actually, we don't have any advantage. Maybe it's because it's more complete in terms of the overall interaction. And the prompt is more convenient for the user. Because when I use other products, many products have built-in a lot of processes and codes, including some prompt. They don't give feedback to the user. This causes problems. When the user sets up the prompt, it's more difficult. Because I don't know what you built-in. If you let me adjust it based on this basis, it will be more difficult. I think our FuzzJPG is now... I think our FuzzJPG is now... I remember when I first experienced it, I remember FuzzJPG has a big advantage. It seems to support the establishment of many... To put it bluntly, it supports the establishment of many... I should say, the knowledge warehouse scenarios. It can be a SaaS system, right? If I remember correctly. It seems that you have a large management background. And if I can deploy this knowledge warehouse to many companies, and it's a split account deployment, I think it seems to have this advantage. I think it's not. It's not. Actually, we haven't done SaaS on the interface yet. We are still planning. It's in the interface. I remember that you have a large background. In addition to the direct login to the knowledge warehouse, there is also a management background. I always thought that the management background was a SaaS direction. Because in fact, this scene is also a question that many people have been asking me recently. I think it's a good question. Yes, in terms of SaaS, we may not be in a hurry to push SaaS now. Because in fact, maybe now pushing SaaS and the future when the structure is changed, SaaS has a lot of full lines that need to be changed. I understand. Okay. Okay, thank you. Anyway, I actually want to ask again. In fact, I have one more advantage in the process of using FaaS. I think I will add one of your advantages. I stand from the perspective of the user. What advantages does the user need? I think the advantage of FaaS GPT is that everyone can expand on it. It's very convenient. Compared to other knowledge warehouse products, on the one hand, the maturity is relatively high. As Mr. Yu mentioned, the maturity of FaaS GPT is relatively high. For example, it can be used directly. Secondly, it is also relatively good in terms of changeability. Because there are high-end labels. These are also the modules that can be dragged. And the third one is that it itself, for example, I want to integrate it into my own system, or even do some small It's very convenient. It's better than other relative overall. You said that it's actually quite suitable. It's in line with the positioning of FaaS GPT. When we are positioning, it's not a product that directly faces the user. It's more of a product in the middle row. I see. So in terms of expansion, we are really going to do a lot of things. Including when introducing high-end labels, we didn't use the Wolfram directly. I think this is the reason why the development of LongTrend is so important. It's also a product that is not suitable for people who don't know LongTrend. In fact, LongTrend Flow has just has put LongTrend into a스트. If you don't know anything about LongTrend, you don't know how to use LongTrend Flow. I don't know how to use it myself. I used the JS version of LongTrend. Many people haven't seen it yet. So I use LongTrend Flow, it's actually very useful. I don't understand how it works. If you don't understand it, it's more difficult to use its可视化产品. So when we set up可视化, we try to consider it as a common user's thinking. As for expansion, it's mainly based on HTTP expansion. HTTP can do a lot of things, or 90% of things can be done. But the only drawback is that it can't be sealed. If it can't be sealed, people who don't know how to write interfaces can't use it. So the second seal, including multiple sealings, is an important direction for us. It includes introducing some cloud products from our own company to create an online interface. It's a quick way. It's also... It's a very important feature for the future. I see. OK. Thank you, Mr. Yu, for talking about FaaS GPT. I think I've already clarified the differences between FaaS GPT and FaaS GPT. I have a relatively clear understanding. OK. Next, let's welcome Mr. Qin of Bisheng to talk about some of Bisheng's advantages. Of course, this topic is also very sharp. I think... I'm standing at the user's point of view, which is quite QA for everyone. I hope you don't feel embarrassed because this is a matter of concern. In fact, when each user is choosing, we will care about which scene is more suitable for each of us. As a manufacturer, we also care about the users' concerns, including their positioning. It's also very important for the users to be able to convey it. Next, please, Mr. Qin. Yes, I think this is a very good question. Because I think the advantage... I don't think it's about the advantages. I think it's about the positioning of the difference. Because our team fund is to serve large enterprises, such as banks and state-owned enterprises. So I think we know more about the needs of large enterprises or enterprises. So from our first day of positioning, our positioning is to serve enterprises. And then it's not to serve C, and then it may not be to serve SaaS. And then we pay attention to the landing of it in the enterprise. This is our differentiating positioning. And then based on this positioning, we will do, for example, the scene in the enterprise will be more complicated than the C-end, and it's different for different enterprises. So here we go to the one that Mr. Yu just mentioned, although the threshold is relatively high, but we will try to do it. Thank you. It's the reason why we decided to do it, because we will try to do it. So its flexibility and freedom is very high. All of its hundreds of components can be used directly. Because we directly reuse it. So there will be a lot of... But right now it's still based on the foreign ecosystem. So we are now also taking some of it, including how we are thinking about how to let everyone participate and make some domestic ecological components. For example, I'll take a TN Tea, take a Qi Cha Cha, take a what? or the interface of domestic stock markets. We can use these things to introduce them into the system. This way, they will have a stronger and comprehensive ability. We are very pleased with this. The second thing I mentioned earlier is that in the enterprise, because our AI applications are all based on data, we can use it to make data processing. In the past, we were the leader in the data processing market. We put our ability here. I believe this is also a point we need to work on. The other thing is that in the enterprise, there are many other things besides the application and efficiency level, such as management demands for enterprises, including high-use and the structure of two-place multi-centre requirements. We will consider these things in the future. This way, we can satisfy the needs of the customers. If you want to use products like B.Sheng to serve your own business and customers, I think this is a good choice. Okay. I would like to add something to what Mr. Qin said. Mr. Qin, I think B.Sheng has a big advantage. The team is from the industry. There used to be a lot of customer scenarios. They have a lot of customers. So they should be in the enterprise level. If you use this directly, you should have a lot of experience. For example, at least B.Sheng's document is the one I currently see, whether it is domestic or foreign. It should be said that it is currently the most perfect document for everyone to understand the various components of long-term. And this includes long flow. Because Mr. Qin said that, it is based on the long flow. So, for long flow, whether it is flowwise or other, its overall introduction of a component, and the combination of a component, the scene combination, should be the most perfect one I have seen so far. The document and the introduction of many scenes are the best. So, if you are interested, you can go down and study it yourself. Okay. This is the scene of the knowledge warehouse we talked about. I think the following words are about the long-term development of the long-term development. The first one is about the current topic, which is the topic of agents. The topic of agents is the current situation. For many people, or many companies, whether it is a digital company, or a company that is currently using the scene, they all want to use agents. For example, Mr. Yu mentioned it earlier. He said that the topic of multi-turn dialogue is to do some sortings, sortings of tasks, which is to become a flow, so that he can split a question into more than ten rounds of dialogue, into three parts, and then three parts may contain different materials, and then build them. In fact, this process itself is a, in our understanding, a small agent. Because it is equivalent to the knowledge warehouse. Generally, my understanding of the knowledge warehouse, you can discuss this. My understanding of the knowledge warehouse is that an outside knowledge content is equivalent to a large volume of information on the large volume. This is generally called a knowledge warehouse. However, if we put an outside person's SOP, or a person's workflow, some are called workflow, some are called SOP, or some people may call this my one, that is to say, a process of logical thinking and reasoning, and combine this thing with the large volume, it is equivalent to saying that the large volume itself is a limited reasoning, especially for many scenarios, it cannot be covered. So at this time, it is necessary to put the SOP of the relevant scenario or the SOP of the relevant scenario or the workflow on this large model. It is equivalent to saying that using a SOP or a workflow workflow to command this large model. You may be in this stage, you play this role and do this work. In the next stage, you play that role and do this work. Maybe this is probably the initial definition of an agent. Then, there should be a lot of limitations for the agent to be constructed in the use. First of all, I want to ask you two to build this one. Do you think the biggest limitation is what? For example, when we are constructing an agent, what agent can be constructed? What constructions are more demanding? I hope that you two... First of all, let's ask Mr. Yu of FuzzJPT to talk about this scenario. Hello, Mr. Yu. The biggest limitation of the most obvious question is that the general message is not good. Every scenario has a... It's a different scenario. Every scenario... The agent in different scenarios is certainly not easy to use. They are actually... So, this one... Because the agent construction is more complicated. It takes a long time to adjust. Especially, if you think about Meta77, it's a very complicated process. So, their construction is not... If you don't have a professional team, you can't construct it. And the problem is that you can't use it for every industry or every user. You can't set up such a set of agents to use. I think this is a big limitation. And it's relatively high-level. The general user... Actually, you can say that by using the kind of code-based code, to construct such an agent, it's actually... It's very complicated. Especially, when it's a chain, when it's a loop, it's actually better to use a code-based form to achieve it. I understand. I understand that, what you're saying is that regarding the agent, a bigger problem is the scenario. For example, the know-how of the scenario is very important. If you don't know much about the scenario, and you're not familiar with the know-how of the scenario, and you're not familiar with the external SOP or workflow of the agent, it's probably a big limitation. Yes, actually, there are many... And there's a premise for constructing agents. You have to understand the scenario. When you don't understand the scenario, you can't write SOP. You don't have to talk about the structure of the agent. I understand. This is actually... I think this is actually... If we talk about the past, it's probably a year before IE. I should say that if you want to start an AI business, you have to have a very high threshold. Basically, most of them are AI doctors, some of them are doing some kind of algorithmic direction, or some of them are playing with some high-level people. But actually, after the appearance of the new model, for those who are doing the scenario, it should be said that the starting threshold of AI has gradually dropped. For those who are doing the scenario, it has created many different opportunities. I think this is a very big opportunity for you. Because it is precisely because of what Mr. Yu said just now, the entire SOP process of the scenario construction, it should be said that the know-how of each industry is currently not available in the large model. So, this is also a kind of advantage that we can build the relevant industry agents through the external scenario, the work flow and SOP of the scenario. And then in the future, we can make a breakthrough in the entire implementation of AGR. I think this is still a very important point. Okay. Mr. Yu, I have another question. I have another question. Do you think that in terms of technical construction, do you think there is any limit in the area of agents? There is no limit. If your SOP is already very clear, the structure should be very simple. It is very simple. If a certain team has a very large map, a flow map, it is very easy to write the AGR. It is very simple to write a scenario AGR. But it is very difficult to write a general one. I see. Okay. I have another question that a netizen asked. Someone just left a message and asked a question. He said, he wants to know the difference between DeFi and FastGPT. Because many internet users put you two together to compare. So, this is... DeFi has a good relationship with us. Yes. Because Mr. Zhang... I forgot to ask him last night. Mr. Zhang, I forgot to ask him. I wanted to ask him to QA. Yes. DeFi... How can I say? Maybe they are a bit out of the box. But I think DeFi is not very familiar with their positioning. Okay. Yes. Maybe... I understand that they are doing pretty well overseas. The overseas market. Yes. From the community's perspective, they are good. But from the business landscape, they might be a bit... hard to cut. Well... You said it's cool. Maybe it's just... you can't find it. And then, the output is a bit... DeFi is not that good. It's a box-to-box system, but it's not that good. The results are not good. It's completely dependent on DeFi itself. It's not dependent on the user. Yes. The user's... The whole micro-challenge is actually relatively small. I understand. Including DeFi, maybe... I think DeFi should still be more like... a coin. There's more SaaS. But... But I personally think... I can't say bad things. I think that... there are a lot of features that are involved at the moment. A lot of features are a bit out of the box. Okay. Yes. But I talked to him. We said the app market is useful. It seems that it's not really useful. And then I thought about it. Maybe the app market is the most... to give you a case. Maybe the app market is now... to be honest, it's now a problem market. Actually, it doesn't have a big relationship with the landing scene. Maybe it's just for the sake of it. Or to play with you. There are more. But I still have to come back. I found... I'll give you an example through the app market. I want you to see how to build this thing. I want you to see how to build this thing. I'll tell you how to do it. I see. So we recently... So we recently cut off the app market. Because maybe... I think the previous app market... didn't really give you too much of a role. Maybe in the future, the app market... will be... When the whole modularization can be sealed, when some app markets are opened, including some feedback from creators, the app market at that time... may have a real role. I see. Actually, I think there is... I have a point. What is it? I feel that... I'll add on this. I think the difference between you and DeFi... is my personal feeling. I'm still standing in the use of this perspective. Of course, I can't say... Because the market is huge. The market is huge. So there is no one to replace someone. Because I know Lu Yizuo. So I can't say anything. How to say it? I think overall, in fact, FastGPT and DeFi... FastGPT's expandability, I still think its expandability is better than DeFi. But I think there is... One is expandability. This is the first one. The second one is FastGPT itself. In terms of the open-endedness, I think it may be better than DeFi. I think these two aspects are the biggest feelings from my point of view as a user. I hope that everyone... can understand it better. And there is another one. In fact, now, I think for the market, the issue of the app market you just mentioned, let me extend it a little bit. The issue of the app market, now, in fact, not only DeFi, but also for MetaGPT 5, everyone is starting to go in the direction of marketplace. It seems to be a parallel. It's the first step. Then, it seems to go up a little bit. Including MetaGPT, I think they also face a problem. Of course, I'm probably observing them. I think their problem is that they have taken the first step, but the second step is not clear. So, they launched this marketplace to collect the market share. But in fact, it seems to be the general effect. Yes. It's indeed a problem. Because the three companies are facing the same problem. We are starting from technology to face the scene, but not from the perspective of the scene to push the technology. So, this is a problem. We have the technology, but the scene on the target may not find the appropriate application scene. Including our team, at first, we thought that this technology may have an effect on many industries. But in fact, when you do it, it's not that easy to use. Yes. On the contrary, we may not be very good at it before. Yesterday, I talked about that they are a very detailed field. It's an agent in the field of e-commerce. They also do some agent work, which is to recruit customers. What are the specific details? But they just push the scene from the scene. They are not a lot of agents. And they have been doing it for almost a year. And now, they are finally on the ground. It really is the first time I have seen such an agent on the ground. I was thinking yesterday, they have been doing such a small scene for almost a year. If we do it in a general way, it may be more difficult. So, we may have to train a little bit later. Yes, it's really... In October last year, they started to do it in October last year. They started to do it. And then, I started to think. And then, in December, they released the GPD, and there were more. So, there are better ways to do it. And then, they did it in August. It's been almost a year. Okay. That's great. And then, they can use it. So, it's such a small scene. It takes so many people to do it for so long. So, I thought about it. So, it's normal to encounter this situation now. Because we are looking at it from a technical point of view. They are looking at the scene from a technical point of view. In fact, we have made a hammer. And then, the user took a nail. Now, he is looking for a bone. Yes. So, we may have to focus on it later. It's a very long-term plan. I decided to focus on it later. I will focus on the Wenda scene. Okay. Okay. Actually, this is also for the development of the industry at a certain stage. We have to focus on the specific landing slowly. In fact, many LMS and OPS platforms in the industry are facing this problem. At first, there was a lot of thunder. Everyone thought it was valuable. But when we found out that it was landing, it wasn't ideal. Or it didn't meet the expectations of the real landing user. So, there was a market bias at this time. The market bias may be due to the platform or technical direction of the explorer and the exploration of the scene direction. The two sides have to be in the middle. In the end, it has to be a more balanced way. Okay. Okay. Next, let's welcome Mr. Qin, who will win. Because I know that Mr. Qin's team came out of the scene. They have a lot of experience in the scene. They used to do the scene themselves. They are very good at the scene direction. Then, Mr. Yu mentioned the scene direction. Maybe because Mr. Qin went from the scene to the OPS platform. Mr. Yu should be more inclined to go from the platform to the scene direction. So, there are two different directions. So, from Mr. Qin's point of view, what do you think about this part of Agent? What are its limitations? What are some of the problems? I would like to hear your opinion. Please, Mr. Qin. Okay. Let me think. We have only seen some scenes in the company. In fact, we are not purely from the scene direction. Because in fact, we all pay attention to the technical trend of the OPS platform. Then, we only saw some possibilities and spaces that it has fallen into. For example, I think one of the advantages of it in the company is that most of its... The text output is more standard. It is not like we are chatting online. You can't control what the other person is saying. Then, in the company, whether I write a report or... How to say it? The main thing is to write a report. Or I go to check a certain information in the company. I think it is more standard than Internet information. I think it is not so difficult when it falls. Then, there are always times you can hold a serious thought about it. What is really special about it is that It's made up of two functions. For example, you can contributed with many opportunities you may not even imagine with it. High activity, Merci, Ad You know that For each focus, Mr. persön density matters more. But that doesn't mean you can create a good result of it. On the contrary, Ryan seed seed employee He task was It's more flexible. It's not a step you write down for it. It thinks for itself what steps it should use. And this thing, you can give it some framework. For example, the most famous React model, it gives this big model a framework to think about. It tells you how to think about and solve problems. It divides it into four steps. The first step is to get this... The first step should be called thought. I get a problem and think about how to solve it. After thought, I judge... It can tell it... It can be default to have some of its own capabilities. For example, it can search or it can take some external tools. In React, there's an important thing called tools. It judges what tools I should use to solve this problem and what parameters I should pass to this tool so that it can provide me some capabilities. This is tools. This is actions. First, I do thought, then I do action. After action, I observe the tools for action. I get the content back. Then I do thought again. What steps should I do next? It's a cycle like this. Then it forms this agent called React. I think this React model is just an agent. I think it consists of two parts. One is its thinking framework. The other is the model itself. Of course, if we go further, it also includes the external tools it can use. It uses some tools. These are the three parts. Then what I see now is that OpenAI models, especially those of SSE, which are used as agents, as you all know, are actually OK in some relatively less developed scenarios. Now I think the main problem is this... For the open source environment, this direction is not mature yet. We haven't spent too much effort to optimize this aspect. I think it may be due to the lack of code data or some data, or some reason. Everyone says that GBG4 is a multi-model combination. Then are these open source models... Because now there is no open source model to be used in this multi-model combination framework. Then is GBG4 a special model to do this? I don't know. So I don't know. So far, the models in these open source environments are not very supportive of the agent's ability. This is a problem. Then the core model is the thinking framework. The thinking framework, like React, is actually a general thinking framework for agents. In addition, for example, we are doing what I just talked about, meta-GBT, including a lot of similar auto-GBT. It's all... It has its own thinking. It defines a set of thinking frameworks. For example, meta-GBT defines, I say, you have multiple agents here. Then what role does each agent have? It defines the thinking framework of each agent. Then these agents work together. It's a multi-agent framework. But its core is still the thinking framework. Do some articles on it. For us, we are thinking more about the thinking framework of each scene in each field. What is it? The SOP that you were discussing just now. How do we direct it in? Then we can directly direct it in better. Then, for example, how to integrate it with this general thinking framework of React? Because you have to start from scratch. Of course, React itself is not complicated. But I think it is a very good thinking framework. We add it on top of it. See how to direct some business know-how. This is what we are experimenting with. There are some initial achievements. Like we are doing some analysis of this recruitment book. We are doing some research on it. We are doing some research on it. We are doing some research on it. We are doing some research on it. Then we are doing some analysis of recruitment books. Then we are doing some analysis of contract reports. I think it is already able to have a good effect. We will introduce some external informants to it. And then, we will let it judge from what information source to what information under different circumstances. And then, we will make a judgment and do a review. This is to say that this agent is divided into three core parts. The thinking framework, the model and the tools. Tools are more about accepting external data. Just like what we just said, we can have an infrastructure ecosystem. I can get all kinds of external information in. And then, and then, the other thing is that these three core parts, and then, I think in the short term, the open source community, the open source ecosystem is doable. It may not have a model as strong and comprehensive as GPC. It can only have multiple. For example, some are specialized. There may even be someone who is specialized in training a model that is specialized in solving the problem of agents. Because it is actually a model that focuses on reasoning, rather than its actual knowledge. Then it will have some specialized models. Then we can use different steps on platforms like Lungflow and Bson. We can use different models to train. Then it can form a... For example, we are testing it now. We know that there is a model called GPC, which is a model that is based on GPC. We know that there is a circle agent. It can directly translate the user's request into some circle language to check data and calculate data. It may be more in the direction of circle. Including your frame is also written in the direction of circle. In fact, you also have a general agent to solve a problem. Then you can combine this agent to do some complicated problems. Like the circle agent, it may be specialized in writing circles, checking circles. Then the general agent can analyze first the user's business intentions. Then based on his knowledge, or whatever, to disassemble his business problems into one by one circle query needs to give to the circle agent. Then the agent will check. Yes, so I think this combination, I think this may be used in the recent open source environment to solve it. Then... Yes, I think I want to say about these. Okay. Actually, for this part of agent, I actually had a thought before. I also want to talk to you about what the idea is. That is, I actually always think there is a trend. In the future, the agent will call three things. Of course, we usually say that it calls tools. But in fact, I think it can also call other agents. So, whether the future can achieve this, for example, I build an agent myself, and then I use another agent to involve this agent in this way. I think this may be for the future. Yes. For example, in the later stage, especially when the marketplace of agents is more mature, this may be a direction that everyone needs to discuss or think about. Because some people may do better on the agent that is divided into small groups. Some people may do better on other agents. They may not need to start from the beginning and then go to the AIG. Maybe it's done with the one that others have done. Then it's good to use. Then this is what I think. I don't know if this is the case. If the future is to achieve this kind of self-construction, and then give it to DIY, I don't know if it will be difficult to achieve this. Because I think it seems that you still have to write code to build an agent for long flow. To be precise, long flow has knowledge. You are talking about this thing. First of all, it can directly connect to the agent. So it can be achieved on my platform. And then, yes, including some of our own internal forces that have not been opened yet. That's how it works. Oh, that's because 5G long flow seems to be in that. Because it's built, for example, I built that agent. It seems to be integrated into that agent. It doesn't have an agent classification. It seems to be written in the classification. Is there any possibility that, for example, after I made it, there is a button on it. I just added it directly. It's automatically added to this side. This kind of strength. Let me think about it. If you want to add a new agent type, then it needs to be added inside. But it has a content similar to the self-defined agent template. You can just write it directly in there. I understand. OK. Because there are many security methods. I believe most people may want to build it on the Keshava interface. Save it. Then, for example, add it to the classification. Then, maybe just pull it out and drop it. Yes, now it has an agent called Zero Shoot. That's what everyone can self-declare. OK. Good. Thank you, Mr. Qin. Next, let's talk about the future development and use of the agent. In fact, the topic of the scene should be the focus of everyone. Although many people talk about agents and the database every day, what kind of scene can this thing be used in? I want to ask two people. For example, because FaaS's GPT and Bsense should serve many customers. So, I think it's a good idea to talk about the future development and use of the agent. For example, for Mr. Yu, FaaS's GPT, you have served so many customers. In fact, what scene do you use it in? And which scenes have you used FaaS's GPT in? From your understanding, what is this? I want to hear your opinion. Mr. Yu, hello. OK. FaaS's GPT is not yet available. There is no agent. It's a complete concept. Currently, it is used in a lot of cases. One is the internal document question. And there are more in the two scenarios of the foreign customer service. And it is also in line with the two scenarios we predicted. And then about the future development of agents, FaaS's GPT may be bigger on agents. I will use agents to do a search. OK. I will use agents to do a data pre-processing. And then we will go to... I am thinking about opening a project to build data, a set of data types that are more close to customer questions. Can I understand this? You mean that in the future, you will have some data processing, some... to solve the problem of recalling, and many other problems, right? Yes. OK. Yes. And the GPT is designed for the question and answer scenario. And its overall recall rate is also relatively high. I see. Actually, there has been a problem in the past. For the model, we all know that the AI customer service itself has been used before. The problem is that the AI is not available. So, it is not talking. But now, after adding this model, it has increased the difference. It is not as much as you think. Of course, the core problem, I think, may still be solved by the following. It is a limited problem of recalling. And the multi-person dialogue mentioned earlier, may be divided into two ways. Or how to do it? For example, you use a number of characters or some other way to solve this problem. This may be the customer scenario, or the internal document that I just mentioned, the recalling scenario. It should be more concerned. Yes. I think you probably understand that we are actually more focused on the question and answer and document-related scenarios in the future of FuzzGP. Yes. Actually, it is easy to understand. Actually, there are many... I don't know if it is e-commerce or tools. In fact, there is already a concept of decision-making. But now, it is all about the human-to-human connection. Yes. So, we will make an optimization for that part later. Yes. I am actually looking forward to it. Don't let people connect. Yes. Actually, I am looking forward to it. Because now, I have served a client before. I know that there are many... For example, I served a customer in a game company before. They were probably not in the business but they had about 100 customers. After using the knowledge library, they cut off the more than 100 people. Then, the remaining people came to fill in the information. In fact, this is a very important part. But in the process of using it, there is actually a problem. It is a practical problem. I want to ask Mr. Yu. For example, when you are asking, you find that the answer is not satisfactory. At this time, you don't know how to deal with it. In fact, there are no particularly good ways to deal with it. For example, the answer is not satisfactory. But I have a satisfactory answer. How can I make this satisfactory answer so that it can quickly stack the satisfactory answer? What do you suggest with this? If the previous GPP is not satisfactory, just reply to it. Just add its question and the final answer. That's it. Now, this can actually be solved. But in fact, it is not solved. In fact, this is not a problem. It is a very basic problem. There is a feedback now. We have feedback and correction. But because we have searched and thought about it, of course, if you ask the same question, it will definitely give you the correct answer. But maybe you change the question, it still can't find it. Yes. Because it is now designed on GPP, it is still a one-sided structure. It is not a tree structure. The problem caused by the one-sided structure is that when the range is too high, actually, there are several ways to increase the feedback. One is to optimize the entire language. Then there is a decrease. This decrease is a very good choice. I can reduce the range of each layer and then do a more round of feedback. In this way, the whole process will be uploaded. And it is also more suitable for most of the current company's personal articles. We will divide the logs into different layers. We will hide some relevant information in different logs. Then we will go down layer by layer. Actually, when we are replying, we can do this. We will go down layer by layer and find it under the corresponding file. This is a bit like the sales agent. There is a sales strategy and then it is digested into the picture under the strategy. Then it is used to match the product. This is quite interesting. It is a small agent. Its difficulty is relatively small. So, of course, the general knowledge is indeed to build a small agent and then to combine several agents. It is a bigger process. This should be a mainstream idea. OK. Thank you, Mr. Yu. Next, we will invite Mr. Qin. Mr. Qin, the application scene of this part of the agent is because of our product, the product of the future. Anyway, I have seen it before. It is in the application scene of the agent. There should be a lot of scenes of DIY. Then, in the future of this aspect, in fact, the scene is in the perspective of your product's original consideration. What do you think what scenes in the future are you very focused on or willing to hope to improve in these aspects? In the direction of product technology, I want to hear your suggestions on the project. Mr. Yu, please. OK, thank you. Let me think. Actually, I don't really have a specific distinction on what scenes can be used for the chain or the knowledge library. And then what scenes can be used for the agent. In our opinion, the core difference is still the difference between the chain and the agent. Chain is my... The flow of writing the death. The agent needs to play it out. He plays it out himself. So he can... Through these two differences, we can see that the scene that can be used for the core is not the flow itself, not the scene of the death flow. So in fact, there are... No matter what kind of business, what kind of department, or what kind of scene, it may be divided into two. One is more intellectualized, and the other is more subjective. I think it's more about thinking from these two points of view. For example, let's take a simple example. For example, a contract, or writing a contract, or a contract review. Some are intellectual model contracts. When I review, I don't have to worry about the content of the model. I only care about whether there is a problem with the filling. We may not need to use the chain, or even the model. I can just write the rules. If you need something that is not standard, for example, a new business, I need to work with a certain manufacturer for the first time, I need a contract with you, then there will be a lot of free stuff in it. Including a lot of things that need to be combined, such as the information of our enterprise, or the information of the outside world, or the information of the other party. But the contract scene, I guess it's not so professional for the current market. Maybe more so, it's in some... how to say... We are now looking at it. We are studying the... The bank, the customer manager, he published a letter of appeal for the credit. There are a lot of content that he can do with Agents. And then... So, it's more about whether you need to call a model or not. So, I think this is the key to the difference between the ability to analyze and the ability to use Agents. And then the specific application form, like to go to the big model, to automatically generate a complete report, you give it some support from the relevant knowledge base, it can be issued. And then, including what we are looking at now, is the process automation. We have some fixed processes, and we can automate it. This is the second one. What I see currently, because in the company, these two types are more business-oriented. And then for the other types of leadership, it's more about doing some data reading. They have more... Traditionally, there will be some... It's the same, the company's death. Right? Our driver's license has been drawn. If you want to change it, you can ask the supplier, how long was the delivery period last year? Why was the delivery time too slow? And so on. So, you don't have to go to any other data collection department to issue such reports. I understand. I have one more question. I want to KO both of you. Let's start with Mr. Qin. I want to ask, regarding Agents, we know that, as you mentioned, some are flexible, and some are not. In terms of the base model, is it affected by the base model? In fact, currently, there is a friend, Jiawei, who is also asking, regarding the domestic and foreign models, what is your advice for the implementation of Agents? When you are choosing the base model to build Agents, what is your advice? Well, in terms of the database, we have not used a large number of data tests like the full-scale test boards. We have used a smaller number of data tests in specific customers or business scenarios. Currently, we have found that the signature is not bad. And then, the public account of Minimax, I forgot the name, but I think they haven't opened it yet. The public account of Minimax. Yes, we have opened the Minimax for their public account. Minimax is relatively good in terms of Agents. But it is just based on our smaller test data. OK. I think your advice is quite good. The test results are similar to what I have learned before. Before, I was working with Mr. Mo, who is the CEO of Agent. He also talked to him. He said that he thinks that Minimax is a more used framework in terms of Agents. This company is also quite interesting. Maybe it is because everyone's data or testing direction is different. So I think in the future, there may be some special tests for your Agents in the open source ecosystem. There is a problem. The tests that we have done are all about the general capacity of the large model. And many domestic models are completely cheating. They are based on the data. You can't control them. They may work under the data. They will be confused when they are out of the scene. This is a big problem. So when you are working on a specific scene, you have to test which one is good and which one is not. This is quite critical. There is no unified standard in this. There is no unified standard in the market. This is a real problem. OK. Of course, there is a friend of mine, Wang Jiahui, who said that Minimax is not good. He said that he has been communicating with them. If you want to go further, you have to test it yourself. Yes. I have heard all the feedback from the agents. I have talked to many agents who have started their own projects. They all have the same feedback. The open-air in the global market is still good. Cloud is almost ready. The cloud in AUS is almost ready. How can I say about LAMA? It is simple. It is a bit complicated. It is not good. It is a bit difficult. This is a phenomenon. There are many problems. We have to make choices in the process. At least, it is difficult to build before the next year. As far as I know, by the end of this year, all the large models in China will reach 3.5. This is also a consensus in the industry. It will take a long time to reach 4. Anyway, it will take a while. I will also ask Mr. Yu about the large model that you mentioned earlier. Especially, the knowledge warehouse. You are more professional. Which large model do you think is more suitable for the FASGP scene? I hope I can give you some suggestions. To be honest, we are not too sensitive to models. FASGP is really good. It is really good. For different models, core is still not the same for their suggestions. The position and expression are not the same. This will lead to some not very good models. It is not easy to design models. But if you want to build a model, I recommend to use GPT-1. Then, you need to train your entire chain. Don't use dozens of lines. The loop of the agent should not be too long. Try to make it more detailed. Maybe you just need to use 4 or 5 rounds of agent. In the future, when you change the model, you can change the model from the point of the line. When you build the agent, you must try to use the best model first. Because the model is developed. If you can run GPT-4 well, then you can use GPT-4 first. You can wait another year and change the model with the best GPT-4 model. I think it is not a problem. OK. I have one more question. During the operation of our branch, for example, when you say that you upload the document, how big do you think it is? Especially when setting the track and the parameters, how should you set it? It is a good question. It should be more direct. For the track, it is not too detailed. It depends on how you build the lock. For example, if you directly separate the block, you can go directly to the 5, 6, 7, 8, and so on. If you need to make some conclusions and create some problems, and create some keywords, then you must make the track a little bigger when you separate the block. Because the lock is small when it is concluded. It depends on how you build the lock. I see. Do you have any suggestions for this? If you want to build the lock, make it a little bigger. If you want to cut the block into small pieces, you can even cut the block into pieces. But it depends on whether your database can handle it. It is a good idea. You can cut the block into pieces. But your database may not be able to handle it. That is the problem. You can cut the block into pieces. There is a combination method. You can choose from five lines and make a complete sentence. This is also a method. But this method is a little bit too much. I see. It depends on your last goal. How to build the lock? How to achieve the goal of searching? And it also has to do with the way you lock. I see. The way you lock is the most suitable. They are not unique. What is the lock method that you think is the best from your experience? The lock method is also a flat lock. I think the best way is to lock in layers and reduce the range of each layer. I see. There is another one. If the database is too large, it is also difficult to build a lock. I see. Regarding the embedding, which one do you think is the best for the open-air? The one that is more open-air. M3E. M3E. What is the worst one? The index embedding. Trash. Trash. You cannot say trash. I'm telling you. In fact, the results that come out are not bad. They are all open-air models. They are all open-air. When they come out, they are not bad. It depends on the data. I see. It is not bad. I think it is a good way to manage data. Okay, let's move on to the last topic, which is the issue of the inaccurate solution of the quantity and the life of the people. I understand that now, everyone has a few ways to deal with it. One is to use the search technology and the quantity combination of ES to search for the ES. Another way is to use the knowledge map and the quantity combination. And the other way is, as Mr. Yu mentioned, to use the knowledge library and other ways to improve the lock-down. So, I would like to ask you two to talk about this issue and what you think about the inaccurate solution of the quantity. Especially, Mr. Qin, who is the winner of the previous Q&A session, had a lot of thoughts on this issue. So, I would like to ask Mr. Yu to take a break and ask Mr. Qin how you think about this issue. I also saw that you have deleted your picture. You can see it here. You can explain it to everyone. Okay. Yes. I think that we mentioned some big strategies in the beginning when we were talking about the data pre-treatment. Because for us, when we look at the data in the enterprise, it is usually dirty. So, we think that the most difficult thing to do is to scan the picture and scan the key. After you have finished the OCR, you have deleted all the information that is broken down. You may have to restore it to its original state. It also has a double-blank and multi-blank file. So, there are a lot of tests for the data pre-treatment. There are various file formats like the common Office 3D model. There are also some more biased ones. You have to be able to handle some file formats. This is a challenge. After you have done that, you have to try to put some original information labels on it. For example, where is the title, where is the label, and what is where. Then, based on this information, you can do the data partition before entering the knowledge library. This part is actually very important. It directly determines when you will recall the above and below text of the model and the content of the model. Then, recalling the data. I think the idea is to integrate various lock-and-drag strategies into one. Or you can support various recall strategies. It is different for different scenarios. For example, if you want to find a book, there are four pages of more than 2,000 pages. There are all kinds of materials and various explanations. It is like a dictionary. It is not effective when you use the lower-down search. Because the things it describes are similar. What is the shape, how to detect it, etc. The most characteristic thing is the name of the drug. So, the ES effect is significantly lower than the lower-down search. So, different scenarios require different lock-and-drag strategies. This is recalling. Then, there is another part that talks about the result after recalling. We can actually do a more detailed process. We don't need to use it all. If we use it all, the model will not know where the key information is. Its attention will be affected. So, we will do some operations to eliminate irrelevant users. I see. I was asking about the picture that I showed you yesterday. You mentioned the difference between Bicheng and LongFloor and 4Wise. Can you also introduce the biggest difference between them? LongFloor and 4Wise are more like global products. They are more transparent. They are not serving a specific type of situation. They are just a long-chain visualization. Their location is more related to long-chain or their ability border. So, they are a universal tool. We will seal it up and make it an enterprise-level landing. This sentence can probably express our core positioning difference. The points below are all details. I will explain the application functions in detail. It is a tool that is more like an experimental test. How do you go online after the experiment? How do you use the users? These are not the questions they consider. And how do you use it in the enterprise? They don't consider it. And how do I connect with the external system? This is another problem I mentioned earlier. How to process all the dirty data and get it into the knowledge base? And another problem is how do I connect with the external data besides the internal data? And the following is similar to the recall problem. They only consider the most common tools. We will consider what to use in each specific situation. We will also provide some customized and selected tools. We will provide some customized tools so that everyone can use them directly in the open-vm. Okay. Thank you, Mr. Qin. Thank you, Mr. Yu. We have been talking for an hour and a half. I will look at the questions that everyone asked. Another question from a friend is how to get the right answer for multiple agents. I will ask Mr. Qin first. How to get the right answer for multiple agents? So far, we have been giving him a task and he will solve it himself. Then I will execute it myself. In the process, I can't intervene him. I can only wait for him to finish. Or you can set a timer and he can only run so many rounds. If he can't run, he will stop. Regarding the problem of multiple agents, I have a tool called Sequence. It has steps. You do this, then that, and this. This should be able to intervene. But we haven't done a lot of experiments on this yet. I haven't had much experience with this problem. I understand. I will ask you about this question. I will ask Mr. Yu first. How do you see this? Do you have any questions? Any attempts? I am sorry to ask this question. We have no idea about the agent's sequence. I understand. I have done some thinking on this before. I can also share it. Regarding the sequence, you can do this. There are several ways. One way is to introduce a second model to do a test on its status or result. For example, we used a method for a lawyer assistant. We might use two models to do a confrontation. What does it mean? After the model gives the result, I will use a cloud to give the result. Then I will use the cloud to give the result to criticize. After two to three times of trial and error, the result will be a big improvement. I can also use a method to do a task plan and let another model to criticize the project or to review it. For example, in our daily work, we do the same as the product and the client. The product manager says I made a prototype of a product. The client says no, and then the client does it again. Then the client does it again. I think this is the method we used to do in practice. We use multiple models to do a PK. Anyway, from the results we have executed before, the result is good. This is a way to do it. Another way is to use a record of the status in the middle. But you have to make sure that every time you use a large model, you have to make sure that the result is executed. Then it will probably meet your expectations. What does that mean? First, you have to estimate that the multi-person dialogue of your large model may support three rounds. If you PK with it, you can't exceed three rounds. There are many estimate conditions. You have to add these estimate conditions. This is the nature of the boundary of the large model. In fact, if you really want to use a large model well, you must have a very clear understanding of its boundary. This is what we are exploring for the entire industry application. But I believe that in the future, this will be more and more clear. For example, just like Mr. Qin said, they may do some exploration and trial in many scenarios. Including mature scenarios, they will be released in the future. Including this, I know that some teams are also doing some test trials in the direction of the edge of the large model. This may come out later. This is about the problem of error. There is another question. I see that there are still people who ask about the text-to-channel. This topic has been discussed before. Jiang Wei, you can look back at the content of the previous discussion. OK. In general, for the knowledge library, agent, there are many very detailed questions. Especially in practice, we have encountered these problems. We have covered almost 90% of these problems. I am also very grateful to Mr. Yu and Mr. Qin for the wonderful sharing of this video brought to you by Mr. Yu and Mr. Qin. Thank you very much for your time during the weekend. Thank you very much to all the friends in front of the screen. That's all for today's live broadcast. Finally, let me introduce AIGC Link. AIGC Link is... I started to launch AIGC Link because I hope to help entrepreneurs and developers who are embracing AIGC during the AI wave wave. I hope to help them and also to inspire those who want to embrace AIGC. I hope to help entrepreneurs who are entrepreneurs. AIGC is a relatively public organization. I hope that in the future, I set a goal for myself. I want to share and explain about 1000 scenarios. If you are in front of the screen, whether you are a startup or a developer, you can communicate with me anytime. I hope that in the future, in the era of AIGC, we can seize many opportunities and move forward together. If you need anything, whether it's funds or the market, you can find me. I can help you. I will do my best to help. Thank you. That's all for today. Thank you, Mr. Qin and Mr. Yu. Thank you. You're welcome. Thank you. We will end here. Have a good day.",
    "msg": "多媒体文件转换文本成功!"
}