{
    "code": 10000,
    "data": "好那这期视频依然是我们商单之后的code review让我们感谢阿里云的大力支持这次code review呢知识星球有人报名所以我们就选了知识星球一位用户在忙我们来简单看一下readme它这个项目叫billipod它的核心功能是把b站上用户的视频或者一个playlist变成一个podcast大概意思呢就是用户给这么样一个configuration的yaml这个yaml里呢你能配置非常多东西对吧然后在这里fees这个地方你可以设置呢每个podcast的这个id然后每一个podcast它是从哪一个用户或者哪一个playlist来的当然下面还有一些乱七八糟的配置啊大概就是这么个意思先完成这个配置然后运行程序这个程序从b站上下载视频转换成podcast那这个项目比较大啊我们只看中间的一部分特别细节的东西呢我们只简单点一下我们主要来聊一下这个架构和流程首先我们看一下它的结构啊它的项目结构呢是采取了这种source的方法然后这个入口文件呢是在sourceblypod然后main.py那一般这种结构呢我们就会把整个这个source的部分打包成一个package大家可以看它实际上也是打包成了一个package它这里有一个init.py但是我们来看一下这import的方式前面这个部分是builtin没有问题接下来是第三方库下面这个部分是local的module但是大家观察一下它的local module是一个local的module使用的是绝对引用这就失去了把你整个项目做成一个package的意义那这种引用方法只有在你的入口程序刚好在这个sourceblypod里面的时候才能工作而我们看它很多其他的库函数也是使用的这种绝对引用也就是说它整个这个package没有办法作为一个真正的package被使用它只能运行那个main.py文件才能跑得起来那如果我们来看这个package呢如果是这样的话你就完全没有必要用source的这种结构你也没有必要把你这个项目做成一个大的package就这种结构和这种写法基本上可以说是冲突的那这里如果你想把它做成一个大的package内部的引用就一定要使用相对引用好我们接着往下看这个main.py前面有个大的banner这里是这个入口然后artparts这些东西我们都不说读完了配置之后直接跑了这个main函数大家注意一下这里用的是asyncio.ru对吧也就是说它整个程序用的是写成在它的程序里也大量的用到这种await我们先来看这个logger给自己的程序打logging是一个非常好的习惯但是当你自己在写一个程序的时候你要意识到logging和正常的interact之间的差别就有些东西你是想记录到log上的但是有些东西你是真的想打印给用户看的有可能你的log本身默认是输入到standard alt但是这两个东西语义是不一样的显示给用户的东西应该有没有log都显示它应该是跟用户互动的必需品而log是可以开启可以关闭可以设置level可以写到另一个文件的我可以想象你想把刚才那个banner展示给用户但是我觉得这个banner不应该属于logging的一部分尽管默认情况下它们都会在你的standard alt输出所以我觉得这里应该有一个区分这边处理化database稍微有点怪这个逻辑大家可以看这里的逻辑是如果这个path的parent也就是这个文件夹不存在就建立这个文件夹如果这个文件夹存在就把这个文件复制一份我不知道shutil是怎么处理这个地方的但是如果这个文件夹存在但是这个文件不存在会发生什么呢就是你这个dbpath本身并不存在你把它复制一份会报错吗我不知道这块但是至少从阅读上来说的话这块逻辑好像有一个断裂如果你是特意这么写的我觉得我会在这块加一个注释说明一下这会怎么样下面tinydb建立两个table没问题这下面用一个现成单独跑了一个server来serve文件这也没什么用我不看下面首先根据用户给的configuration初始化了一下各种table在这里基本上就是写上我需要去爬哪些东西然后做哪些podcast下面新建了一个asyncio task叫update episodes这个update episodes等会我们会看到它是更新我们要爬什么的最后对于所有需要爬的podcast用schedule job做一个定时也就是每隔一段时间就跑一下这个任务这个schedule job等会我们也会看到最后它又跑了一个现成来support它之前那个schedule job结构大致是这样我们看中间的几个函数从这个中间的函数来看从这个data initialize开始server的部分我们不管我们就看怎么从config变到了pod从我设置的这些fees里面拿到id跟config拿完一个歇0.5秒这个sleep的位置非常诡异就一般情况下我们sleep都是为了防止爬虫过载对吧就是我们担心不断的给一个网站发请求然后这个网站会把我们block掉但是从理论上说你是不是应该先干活再sleep你不能先休息再干活对吧当然如果for loop长了这两个是差不多的第二对于asyncIO来说如果你的程序写对了那你是没有办法通过简单的sleep来防止爬虫过载的因为sleep 0.5不代表在这半秒之内不要给这个网站发http request在这0.5秒之内还有其他的task可以干这件事当然它整个这个asyncIO用的不对所以这个sleep还真是有效的这个我们等会再说然后下面的代码我读起来就开始有点费劲了它首先在这里建立了一个pod object然后又根据这个pod的uid拿到了这个pod info这个getpod info是有网络通讯的所以这块有await紧接着它又用这个pod info去更新了这个pod大家有没有感觉到这块逻辑特别别扭就它展示了很多中间步骤和中间量导致这个for loop很难读我个人的感觉是这个for loop里应该只有这么一行东西就我的目的是从一个feed拿到一个pod就这么简单所以我的loop里就是从一个feed拿到一个pod至于怎么从feed拿到一个pod这是那个函数的事了这样你就能避免在几个数据结构之间来回来去的传数据这个部分是我读着比较费劲的然后它这块还有一个点好像有问题就是它拿到了pod之后它是insert回这个pod table的我们回过头看它建立database的时候它这个database并不一定是全新的当然如果每次都是全新的这个database有没有意义了对吧也就是说这个database里面有可能保存着之前运行时候的数据或者说在之前运行的时候这个pod table里面可能已经有这个pod了所以尽管你是在data initialize这个看似在初始化的函数里你依然不应该把这个pod无脑地给塞进这个table里这块呢可能用tinydb给一个updates函数会更好就是如果这个pod已经在这个table里了就update否则的话insert不然的话如果你有重复的pod那你是不是还得有重复的episode那就很麻烦了接下来根据刚才拿到的pod也就是每个用户或者playlist下面所有的视频信息得到一个episode list就是我要下载哪些视频然后开始下载当然这块还是有一样的问题对吧既然之前可能已经有数据了那我有一些episode可能已经下载过了或者说有的pod我已经处理过了那这里是不是有可能造成重复的下载我还特意看了一下它的这个get episodes list并没有任何的filter就是这个pod里面有多少episode就返回多少episode后面这个部分好像是跟那个web server有关的我们跳过这个就是data initialize的全部内容我不知道大家有没有发现一个问题这个问题是我觉得它整个项目最大的问题大家有没有意识到他刚才完全没有用上ebook这件事他写了很多async away对吧但是我们说写成ebook的核心是什么是多任务并行对吧当我们等待一个任务当这个任务正在做io的时候要有另外的任务可以做这才叫ebook才叫写成对吧而到目前为止他并没有建立第二个任务我们看他刚才没有任何一个create task包括他的这个get pod info里面他的网络通讯也是只有await下面这里依然是线性的等待下载视频的时候用的是httpx依然是线性的他没有建立一个新的任务他所有下载的代码都是这样的他直接await download url也就是说在他initialize的过程中从头到尾都只有一个写成任务在运行当他进行io操作的时候并没有第二个任务可以补上所以我们可以看到这一段看似ebook的代码实际上是完全同步的就他没有利用到ebook的任何优势他的整个代码库唯一一次使用到create task就是这里他建立这个task是update episodes然而我们来看一下这个update episodes他的核心函数体是一个while true而在每一个while true里面他wait一个event这个event是每一次完成这个update pod之后set的而update pod跟我们说的一样跟我们刚才讲的initialization差不多他基本上是更新一下episodes的信息更新一下需要下载哪些新的视频了而这个update pod是什么时候运行的呢是我们刚才介绍的这个地方他有一个schedule job对于每一个podcast他隔一段时间就跑一下这个update pod我们来看一下这个schedule job的实现这个schedule job用了一个第三方库schedule这个schedule是怎么工作的呢?还记不记得这个schedule job的实现呢?记不记得我们当时开了一个新线程跑run scheduler在这个run schedule里面就是这个schedule运行的地方我们有另外一个线程然后schedule这个库不停的在检查有没有新的需要运行的工作如果到点了他就会跑一下这个run async大家注意一下这个run async每一次run async他新建了一个event loop在这个event loop里面跑这个job而我们知道写成的上下文交换或者说是控制权交换是只能在同一个event loop里进行的所以在这个run async的过程中是没有写成级别的上下文交换的每一个run async跑一个update podupdate pod运行完set一个update event然后在主线程这里等待的update episodes他在里面去做更新做下载而所有的通过schedule job发起而所有的通过schedule job发起的任务实际上都是在另外一个线程的另外一个event loop里面进行的他们两件事之间没有任何写成级别的上下文切换只有线程级别的上下文切换而唯一一个和这个update episodes有写成级别上下文切换的点在哪呢在这里这是主线程的那个event loop唯一还在运行的task这个task的任务就是等着而我们分析了这么多就是想告诉大家他写的这个写成程序是一个同步程序他几乎没有任何写成级别的异步他几乎没有任何写成级别的异步他确实做到了一些异步但是是通过线程来完成的也就是说他其实完全没有必要把主程序搞得这么复杂把主程序搞得这么复杂他主程序可以搞成完全同步的他主程序可以搞成完全同步的和现在效果是一样的所以写成这个部分啊大家如果用的话还是要谨慎一点大家如果用的话还是要谨慎一点就你最好要知道你在干什么就你最好要知道你在干什么否则的话你把你程序代码否则的话你把你程序代码弄得很难读还没有利用上这个写成本身的优势还没有利用上这个写成本身的优势那就亏了那这个代码内容呢说实话比较多比较复杂啊说实话比较多比较复杂啊我们一次code review肯定是没有办法把所有代码肯定是没有办法把所有代码全都看完所以我就挑了几个我觉得比较适合讲的点来讲所以我就挑了几个我觉得比较适合讲的点来讲那我们还是希望在code review里给大家带来一些更深的思考给大家带来一些更深的思考就不仅仅是你这儿少个空格就不仅仅是你这儿少个空格那块命名不对而是在代码的结构逻辑啊而是在代码的结构逻辑啊甚至设计上能给大家带来一些启发好那这期视频就到这里吧好那这期视频就到这里吧喜欢的话记得点一下关注别忘了一键三连哦",
    "msg": "多媒体文件转换文本成功!"
}